\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}
\geometry{
    a4paper,
    total={160mm,250mm},
    left=20mm,
    top=20mm,
}
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}   % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
						% TeX will automatically convert eps --> pdf in pdflatex		

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{tensor}

%SetFonts


\title{Levi-Civita-Symbol a.k.a. epsilon symbol}
\author{Michael Gehricke}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\tenmix}[3]{{#1}^{#2}_{#3}}
%\equiv
We will proof this very useful identity:
\begin{equation} \label{EpsilonDet}
    \sum_{k=1}^n \epsilon_{i_1\;i_2\;\ldots\; i_{n-1}\; k} \epsilon_{j_1\;j_2\;\ldots\; j_{n-1}\; k}
    =
    \begin{vmatrix}
    \delta_{i_1 j_1} & \delta_{i_1 j_2} & \delta_{i_1 j_3}  & \cdots & \delta_{i_1 j_{n-1}} \\
    \delta_{i_2 j_1} & \delta_{i_2 j_2} & \cdots            & \cdots & \delta_{i_2 j_{n-1}} \\
    \cdots           & \cdots           & \cdots            & \cdots & \cdots               \\
    \delta_{i_{n-1}j_1} & \delta_{i_{n-1} j_2} & \cdots     & \cdots & \delta_{i_{n-1} j_{n-1}} \\
    \end{vmatrix}   
\end{equation}
%
We begin by considering the euclidean standard basis $B$ in $\mathbb{R}^3$
\begin{equation}
    \mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},\quad
    \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},\quad
    \mathbf{e}_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{equation}
The metric of $B$ is $g_{ij} = \mathbf{e}_i\mathbf{e}_j = \delta_{ij}$, the Kronecker-Delta.
Which is another way of saying that its an orthonormal basis. Let's have a look at the determinants of B.
\begin{equation}
    \begin{vmatrix} \mathbf{e}_i\;\mathbf{e}_j\;\mathbf{e}_k \end{vmatrix}
    = 
    \begin{cases}
        0 & \hbox{if}\quad i=j,\; i=k,\hbox{or}\; \; j=k \\
        1 & \hbox{if}\quad ijk \in \{123, 231, 312\} \\
        -1 & \hbox{if}\quad  ijk \in \{132, 213, 321\}
    \end{cases}
\end{equation}
Note: Flipping the order of two neighbouring indices (first and last are neighbours) flips the sign of the 
determinant. These are exactly the properties of the epsilon symbol.
\begin{equation}
    \epsilon_{ijk} = \begin{vmatrix} \mathbf{e}_i\;\mathbf{e}_j\;\mathbf{e}_k \end{vmatrix}
\end{equation}
A product of two epsilon symbols becomes just a product of two determinants:
\begin{equation}
    \epsilon_{ijk}\epsilon_{lmn} = \begin{vmatrix} \mathbf{e}_i\;\mathbf{e}_j\;\mathbf{e}_k \end{vmatrix}
        \cdot \begin{vmatrix} \mathbf{e}_l\;\mathbf{e}_m\;\mathbf{e}_n \end{vmatrix}
\end{equation}
Note: the determinant of a square matrix is equal to the determinant of its transpose. Further is the 
determinant of the product of two square matrices equal to the product of the determinants of these
matrices: $\det AB = \det A \det B$.
Hence:
\begin{equation}
    \begin{vmatrix} (\mathbf{e}_i\;\mathbf{e}_j\;\mathbf{e}_k)^t
    (\mathbf{e}_l\;\mathbf{e}_m\;\mathbf{e}_k) \end{vmatrix}
    = \det 
    \begin{pmatrix}
        \delta_{il} & \delta_{im} & \delta_{ik} \\
        \delta_{jl} & \delta_{jm} & \delta_{jk} \\
        \delta_{kl} & \delta_{km} & \delta_{kk} \\
    \end{pmatrix}
    = \det
    \begin{pmatrix}
        \delta_{il} & \delta_{im} & 0 \\
        \delta_{jl} & \delta_{jm} & 0 \\
        0              & 0           & 1 \\
    \end{pmatrix}
\end{equation}
Note that for matrix multiplication we have two repeat one index in each matrix to get proper 
row-column products. \\
\\
The zeroes in the last row and the last column arise from the fact that we set $i \neq j$, $j \neq k$
and $ l \neq k$, $m \neq k$. Otherwise there is not much to calculate because per definition 
$\epsilon_{ijk} = 0$ and $\epsilon_{lmk} = 0$ if one index is repeated. \\
\\
So we get:
\begin{equation}
    \epsilon_{ijk}\epsilon_{lmk} = \delta_{il} \delta_{jm} - \delta_{im} \delta_{jl}
\end{equation}
A generalization to n indices leads to equation (\ref{EpsilonDet}).
%
\subsection{Useful example}
Its very convenient to write vector equations in tensor notation. One important fact to notice
is that the Kronecker-Delta acts as an index exchange operator.
\begin{equation}
    \delta_i^j\;a^i = a^j \quad \text{because it leaves only terms where} \; i = j
\end{equation}
So in a sense the epsilon symbol and the Kronecker-Delta are tools for index shuffling.
More often than not called index gymnastics.
\begin{equation}
    \begin{aligned}
        \mathbf{a} \cdot \mathbf{b} &= a^i b^i \equiv \sum_{i=1}^n a^i b^i  \\
        \mathbf{a} \times \mathbf{b} &= \epsilon_{ijk} \; a^i b^j \; \mathbf{e}_k \\
        [\mathbf{a} \times \mathbf{b}]^k &= \epsilon_{ijk} \; a^i b^j \\
        (\mathbf{a} \times \mathbf{b}) \times \mathbf{c} &= \epsilon_{klm} \; (\epsilon_{ijk} \; a^i b^j) c^l \; \mathbf{e}_m \\
            &= \epsilon_{klm} \; \epsilon_{ijk} \; a^i b^j c^l \; \mathbf{e}_m \\
            &= (\delta_{il}\;\delta_{jm} - \delta_{jl}\;\delta_{im}) a^i b^j c^l \; \mathbf{e}_m \\
            &= (\delta_{il}\;\delta_{jm}\;a^i b^j c^l - \delta_{jl}\;\delta_{im} a^i b^j c^l) \; \mathbf{e}_m \\
            & \text{Now we group fitting indices together.} \\
            &= [(\delta_{il}\;a^i)\;(\delta_{jm}\;b^j)\;c^l - (\delta_{jl}\;b^j)\;(\delta_{im} a^i)\;c^l] \; \mathbf{e}_m \\
            &= (a^l\;b^m\;c^l - b^l\;a^m\;c^l) \; \mathbf{e}_m \\
            &= (\mathbf{a}\mathbf{c})\mathbf{b} - (\mathbf{b}\mathbf{c})\mathbf{a} \\
    \end{aligned}
\end{equation}
Note: It does not make any difference for the epsilon symbol or the Kronecker-Delta if we 
write indices up or down. We would have to multiply the components by the metric tensor or 
its inverse respectively, if we decided to do so, which is in both cases the identity matrix.
\paragraph{}
Happy index gymnastics!
\end{document}